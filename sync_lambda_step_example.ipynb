{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Pipelines: train a Hugging Face model, deploy it with a Lambda step synchronously\n",
    "This notebook demonstrates how to use SageMaker Pipelines to train a Hugging Face NLP model and deploy it using a Lambda function invoked by a SageMaker Pipelines Lambda step. The SageMaker integration with Hugging Face makes it easy to train and deploy advanced NLP models. A Lambda step in SageMaker Pipelines enables you to easily do lightweight model deployments and other serverless operations.\n",
    "\n",
    "In this example use case, the Hugging Face model is trained on the IMDb movie reviews dataset. The goal is to predict the sentiment of the movie review (positive or negative). The pipeline built in this notebook covers the full end-to-end workflow, from preparing the dataset, to model training, evaluation, registration (if model quality passes a test), and deployment.\n",
    "\n",
    "Prerequisites:\n",
    "\n",
    "- Make sure your notebook environment has IAM managed policy AmazonSageMakerPipelinesIntegrations as well as AmazonSageMakerFullAccess\n",
    "- For SageMaker Studio, use the kernel Python 3 (Data Science)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by updating the SageMaker SDK, and importing some necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.20.51 requires botocore==1.21.51, but you have botocore 1.22.7 which is incompatible.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install sagemaker --quiet --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3 # boto3 is the AWS SDK for Python\n",
    "import os # provides functions for interacting with the operating system\n",
    "import numpy as np # library that offers support for large multi-dimensional arrays\n",
    "import pandas as pd # for data analysis and manipulation\n",
    "import sagemaker # SageMaker Python SDK for training and deploying machine learning models on Amazon SageMaker.\n",
    "import sys # provides access to some variables and functions used or maintained by the interpreter\n",
    "import time # provides various time-related functions\n",
    "\n",
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterString # variables that accept string and integer types for pipeline definition\n",
    "from sagemaker.workflow.lambda_step import (\n",
    "    LambdaStep, # Constructs a lambda step for workflow\n",
    "    LambdaOutput, # List of outputs from the lambda function\n",
    "    LambdaOutputTypeEnum, # Specifies type of Lambda output\n",
    ")\n",
    "from sagemaker.lambda_helper import Lambda # Constructs a Lambda instance. This instance represents a Lambda function and provides methods for updating, deleting and invoking the function.\n",
    "\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor # Handles Amazon SageMaker processing tasks for jobs using scikit-learn\n",
    "\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput # accepts parameters that specify an Amazon S3 input/output for a processing job and provides a method to turn those parameters into a dictionary.\n",
    "from sagemaker.workflow.steps import CacheConfig, ProcessingStep # to enable caching in pipeline workflow / processing step for workflow\n",
    "\n",
    "from sagemaker.huggingface import HuggingFace, HuggingFaceModel # used to create HuggingFace estimator / model to register\n",
    "\n",
    "from sagemaker.inputs import TrainingInput # Create a definition for input data used by an SageMaker training job\n",
    "from sagemaker.workflow.steps import TrainingStep # Training step for workflow\n",
    "\n",
    "from sagemaker.processing import ScriptProcessor # handles Amazon SageMaker Processing tasks for jobs using a machine learning framework, which allows for providing a script to be run as part of the Processing Job\n",
    "from sagemaker.workflow.properties import PropertyFile # Provides a property file struct\n",
    "from sagemaker.workflow.step_collections import CreateModelStep, RegisterModel # CreateModel step / Register Model step collection for workflow \n",
    "\n",
    "from sagemaker.workflow.conditions import ConditionLessThanOrEqualTo # A condition for less than or equal to comparisons\n",
    "from sagemaker.workflow.condition_step import ( \n",
    "    ConditionStep, # Conditional step for pipelines to support conditional branching in the execution of steps\n",
    "    JsonGet # Get JSON properties from PropertyFiles\n",
    ")\n",
    "\n",
    "from sagemaker.workflow.pipeline import Pipeline, PipelineExperimentConfig # Pipeline for workflow\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables # Pipeline execution variables for workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll perform some setup for SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = sagemaker.Session().boto_region_name # Get info of which region we are using on Amazon SageMaker (i.e. 'eu-central-1')\n",
    "sm_client = boto3.client(\"sagemaker\") # instantiate API for creating and managing Amazon SageMaker resources\n",
    "boto_session = boto3.Session(region_name=region) # Create a boto3 session with defined region in which we want to create new connections\n",
    "sagemaker_session = sagemaker.session.Session( # Initialize a SageMaker Session\n",
    "    boto_session=boto_session, # The underlying boto3 session which AWS service calls are delegated to\n",
    "    sagemaker_client=sm_client # Client which makes Amazon SageMaker service calls other than InvokeEndpoint\n",
    ")\n",
    "\n",
    "role = sagemaker.get_execution_role() #  Access the execution role\n",
    "\n",
    "bucket = sagemaker_session.default_bucket() # The default Amazon S3 bucket used by this session\n",
    "\n",
    "# define S3 prefix\n",
    "s3_prefix = \"hugging-face-pipeline-demo\"\n",
    "base_job_prefix = s3_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-eu-central-1-910022457801'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::910022457801:role/service-role/AmazonSageMaker-ExecutionRole-20210409T122209'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameterizing the pipeline\n",
    "Before defining the pipeline, it is important to parameterize it. Almost any aspect of a SageMaker Pipeline can be parameterized, including instance types and counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing step parameters\n",
    "processing_instance_type = ParameterString(name=\"ProcessingInstanceType\", default_value=\"ml.c5.2xlarge\") # define processing instance type\n",
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1) # define processing instance count\n",
    "\n",
    "# training step parameters\n",
    "training_instance_type = ParameterString(name=\"TrainingInstanceType\", default_value=\"ml.p3.2xlarge\") # define training instance type\n",
    "training_instance_count = ParameterInteger(name=\"TrainingInstanceCount\", default_value=1) # define training instance count\n",
    "\n",
    "# endpoint parameters\n",
    "endpoint_instance_type = ParameterString(name=\"EndpointInstanceType\", default_value=\"ml.g4dn.xlarge\") # define endpoint instance type\n",
    "\n",
    "output_destination = \"s3://{}/{}/data\".format(bucket, s3_prefix) # data destination on S3\n",
    "cache_config = CacheConfig(enable_caching=False, expire_after=\"30d\") # disable caching\n",
    "model_package_group_name = \"HuggingFaceModelPackageGroup\" # where the model will be registered "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation step\n",
    "A SKLearn Processing step is used to invoke a SageMaker Processing job with a custom python script - preprocessing.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate SKLearnProcessor\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\", # The version of scikit-learn\n",
    "    instance_type=processing_instance_type, # Type of EC2 instance to use for processing\n",
    "    instance_count=processing_instance_count, # The number of instances to run the Processing job with\n",
    "    base_job_name=base_job_prefix + \"/preprocessing\", # Prefix for processing name\n",
    "    sagemaker_session=sagemaker_session, # Session object which manages interactions with Amazon SageMaker APIs and any other AWS services needed\n",
    "    role=role, # An AWS IAM role name or ARN\n",
    ")\n",
    "\n",
    "# Define processing step\n",
    "step_process = ProcessingStep(\n",
    "    name=\"ProcessDataForTraining\", # The name of the processing step\n",
    "    cache_config=cache_config, # to use or not caching (A sagemaker.workflow.steps.CacheConfig instance)\n",
    "    processor=sklearn_processor, # use sklearn processor defined above (A sagemaker.processing.Processor instance) \n",
    "    outputs=[ # 3 outputs (train, test and validation). A list of sagemaker.processing.ProcessorOutput instances\n",
    "        ProcessingOutput(\n",
    "            output_name=\"train\", # The name for the processing job output\n",
    "            destination=\"{}/train\".format(output_destination), # The destination of the output\n",
    "            source=\"/opt/ml/processing/train\", # The source for the output\n",
    "        ProcessingOutput(\n",
    "            output_name=\"test\", # The name for the processing job output\n",
    "            destination=\"{}/test\".format(output_destination), # The destination of the output\n",
    "            source=\"/opt/ml/processing/test\", # The source for the output\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"validation\", # The name for the processing job output\n",
    "            destination=\"{}/test\".format(output_destination), # The destination of the output\n",
    "            source=\"/opt/ml/processing/validation\", # The source for the output\n",
    "        ),\n",
    "    ],\n",
    "    code=\"./scripts/preprocessing.py\", # preprocessing script \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training with the SageMaker Hugging Face Estimator\n",
    "Use SageMaker's Hugging Face Estimator class to create a model training step for the Hugging Face DistilBERT model. Transformer-based models such as the original BERT can be very large and slow to train. DistilBERT, however, is a small, fast, cheap and light Transformer model trained by distilling BERT base. It reduces the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster.\n",
    "\n",
    "As a first step to constructing the estimator for our model training job, we'll look up the Hugging Face container for our AWS Region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "763104351884.dkr.ecr.eu-central-1.amazonaws.com/huggingface-pytorch-training:1.7.1-transformers4.6.1-gpu-py36-cu110-ubuntu18.04\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a Docker container\n",
    "container = sagemaker.image_uris.retrieve(framework=\"huggingface\", # The name of the framework or algorithm\n",
    "                                          region=boto3.Session().region_name, # The AWS region\n",
    "                                          version=\"4.6.1\", # The framework or algorithm version\n",
    "                                          py_version=\"py36\", # The Python version\n",
    "                                          base_framework_version=\"pytorch1.7.1\", # The base framework version\n",
    "                                          instance_type=\"ml.p3.2xlarge\", # The SageMaker instance type\n",
    "                                          image_scope=\"training\", # The image type, i.e. what it is used for. Valid values: “training”, “inference”, “eia”. If accelerator_type is set, image_scope is ignored.\n",
    "                                          container_version=\"cu110-ubuntu18.04\") # the version of docker image\n",
    "\n",
    "print(container) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the container specified above, the Hugging Face estimator also takes hyperparameters as a dictionary. The training instance type and size are pipeline parameters that can be easily varied in future pipeline runs without changing any code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters for HuggingFace estimator\n",
    "hyperparameters = {\n",
    "    \"model_name\": \"distilbert-base-uncased\", # name of the model\n",
    "    \"train_batch_size\": 32, # size of training batches\n",
    "    \"epochs\": 1, # mumber of epochs\n",
    "}\n",
    "\n",
    "# Construct the HuggingFace estimator\n",
    "estimator = HuggingFace(\n",
    "    image_uri=container, # the estimator will use this image for training and hosting\n",
    "    entry_point=\"train.py\", # use train.py script for training\n",
    "    source_dir=\"./scripts\", # Path to a directory for training script\n",
    "    base_job_name=base_job_prefix + \"/training\", # Prefix for training name \n",
    "    instance_type=training_instance_type, # Type of EC2 instance to use for training\n",
    "    instance_count=training_instance_count, # The number of instances to run the Training job with\n",
    "    role=role, # An AWS IAM role name or ARN\n",
    "    transformers_version=\"4.6.1\", # Transformers version you want to use for executing your model training code\n",
    "    pytorch_version=\"1.7.1\", # PyTorch version you want to use for executing your model training code\n",
    "    py_version=\"py36\", # Python version you want to use for executing your model training code\n",
    "    hyperparameters=hyperparameters, # Hyperparameters that will be used for training\n",
    "    sagemaker_session=sagemaker_session, # Session object which manages interactions with Amazon SageMaker APIs and any other AWS services needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training step\n",
    "step_train = TrainingStep(\n",
    "    name=\"TrainHuggingFaceModel\",\n",
    "    estimator=estimator, # pass the estimator that is constracted above\n",
    "    inputs={ # we have training and test inputs\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"train\"\n",
    "            ].S3Output.S3Uri # Defines the location of s3 data to train on\n",
    "        ),\n",
    "        \"test\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"test\"\n",
    "            ].S3Output.S3Uri # Defines the location of s3 data to train on\n",
    "        ),\n",
    "    },\n",
    "    cache_config=cache_config, # to use caching or not (A sagemaker.workflow.steps.CacheConfig instance)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation step\n",
    "A ProcessingStep is used to evaluate the performance of the trained model. Based on the results of the evaluation, either the model is created, registered, and deployed, or the pipeline stops.\n",
    "\n",
    "In the training job, the model was evaluated against the test dataset, and the result of the evaluation was stored in the model.tar.gz file saved by the training job. The results of that evaluation are copied into a PropertyFile in this ProcessingStep so that it can be used in the ConditionStep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create processor for evaluation\n",
    "script_eval = ScriptProcessor(\n",
    "    image_uri=container, # The URI of the Docker image to use for the processing jobs\n",
    "    command=[\"python3\"], # The command to run, along with any command-line flags\n",
    "    instance_type=processing_instance_type, # The type of EC2 instance to use for processing\n",
    "    instance_count=1, # The number of instances to run a processing job with\n",
    "    base_job_name=base_job_prefix + \"/evaluation\", # Prefix for processing name\n",
    "    sagemaker_session=sagemaker_session, # Session object which manages interactions with Amazon SageMaker and any other AWS services needed\n",
    "    role=role, # An AWS IAM role name or ARN\n",
    ")\n",
    "\n",
    "# Define report as json file\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"HuggingFaceEvaluationReport\", # The name of the property file for reference with JsonGet functions\n",
    "    output_name=\"evaluation\", # The name of the processing job output channel\n",
    "    path=\"evaluation.json\", # The path to the file at the output channel location\n",
    ")\n",
    "\n",
    "# Construct evaluation step \n",
    "step_eval = ProcessingStep(\n",
    "    name=\"HuggingfaceEvalLoss\", # The name of the processing step\n",
    "    processor=script_eval, # Use script processor defined above. A sagemaker.processing.Processor instance\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=step_train.properties.ModelArtifacts.S3ModelArtifacts, # The source for the input. If a local path is provided, it will automatically be uploaded to S3 under: “s3://<default-bucket-name>/<job-name>/input/<input-name>”.\n",
    "            destination=\"/opt/ml/processing/model\", # The destination of the input\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"evaluation\", # The name of the output\n",
    "            source=\"/opt/ml/processing/evaluation\", # The source for the output\n",
    "            destination=f\"s3://{bucket}/{s3_prefix}/evaluation_report\", # The destination of the output \n",
    "        ),\n",
    "    ],\n",
    "    code=\"./scripts/evaluate.py\", # script for evaluation \n",
    "    property_files=[evaluation_report], # A list of property files that workflow looks for and resolves from the configured processing output list\n",
    "    cache_config=cache_config, # to use caching or not (A sagemaker.workflow.steps.CacheConfig instance)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the model\n",
    "The trained model is registered in the Model Registry under a Model Package Group. Each time a new model is registered, it is given a new version number by default. The model is registered in the \"Approved\" state so that it can be deployed by the Lambda function. Registration will only happen if the output of the ConditionStep is true, i.e, the metrics being checked are within the threshold defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HuggingFaceModel \n",
    "model = HuggingFaceModel(\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts, # The Amazon S3 location of a SageMaker model data .tar.gz file\n",
    "    role=role, # An AWS IAM role specified with either the name or full ARN\n",
    "    transformers_version=\"4.6.1\", # Transformers version for executing model training code\n",
    "    pytorch_version=\"1.7.1\", # PyTorch version for executing inference code\n",
    "    py_version=\"py36\", # Python version for executing model training code\n",
    "    sagemaker_session=sagemaker_session, # Session object which manages interactions with Amazon SageMaker and any other AWS services needed\n",
    ")\n",
    "\n",
    "# Register the model \n",
    "step_register = RegisterModel(\n",
    "    name=\"HuggingFaceRegisterModel\", # The name of the register step.\n",
    "    model=model, # A PipelineModel object that comprises a list of models which gets executed as a serial inference pipeline or a Model object\n",
    "    content_types=[\"application/json\"], # The supported MIME types for the input data\n",
    "    response_types=[\"application/json\"], # The supported MIME types for the output data\n",
    "    inference_instances=[\"ml.g4dn.xlarge\", \"ml.m5.xlarge\"], # A list of the instance types that are used to generate inferences in real-time\n",
    "    transform_instances=[\"ml.g4dn.xlarge\", \"ml.m5.xlarge\"], # A list of the instance types on which a transformation job can be run or on which an endpoint can be deployed\n",
    "    model_package_group_name=model_package_group_name, # The Model Package Group name\n",
    "    approval_status=\"Approved\", # Model Approval Status, values can be “Approved”, “Rejected”, or “PendingManualApproval”\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lambda step for model deployment\n",
    "The SageMaker SDK provides a Lambda helper class that can be used to create a Lambda function. This function is provided to the Lambda step for invocation via the pipeline. Alternatively, a predefined Lambda function can be provided to the Lambda step.\n",
    "\n",
    "The SageMaker Execution Role requires the policy AmazonSageMakerPipelinesIntegrations to create the Lambda function, and the Lambda function needs a role with policies allowing creation of a SageMaker endpoint.\n",
    "\n",
    "A helper function in iam_helper.py is provided to create the Lambda role. To use the script, the notebook execution role must include the required policy to create an IAM role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lambda_deployer.py\n"
     ]
    }
   ],
   "source": [
    "# this creates lambda_deployer.py script and fills it with everything inside this box\n",
    "%%writefile lambda_deployer.py\n",
    "\n",
    "\"\"\"\n",
    "This Lambda function creates an Endpoint Configuration and deploys a model to an Endpoint. \n",
    "The name of the model to deploy is provided via the `event` argument\n",
    "\"\"\"\n",
    "\n",
    "import json # to create request body in json format\n",
    "import boto3 # to communicate with AWS (sagemaker)\n",
    "\n",
    "# Define lambda function which creates a SageMaker endpoint\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\" \"\"\"\n",
    "    sm_client = boto3.client(\"sagemaker\") # interface with SageMaker\n",
    "\n",
    "    # The name of the model created in the Pipeline CreateModelStep\n",
    "    model_name = event[\"model_name\"]\n",
    "    model_package_arn = event[\"model_package_arn\"]\n",
    "    endpoint_config_name = event[\"endpoint_config_name\"]\n",
    "    endpoint_name = event[\"endpoint_name\"]\n",
    "    role = event[\"role\"]\n",
    "\n",
    "    container = {\"ModelPackageName\": model_package_arn}\n",
    "\n",
    "    # Create model with provided model name, execution role and container\n",
    "    create_model_response = sm_client.create_model(\n",
    "        ModelName=model_name, ExecutionRoleArn=role, Containers=[container]\n",
    "    )\n",
    "\n",
    "    # Create config for endpoint\n",
    "    create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "        EndpointConfigName=endpoint_config_name,\n",
    "        ProductionVariants=[\n",
    "            {\n",
    "                \"InstanceType\": \"ml.m5.xlarge\",\n",
    "                \"InitialVariantWeight\": 1,\n",
    "                \"InitialInstanceCount\": 1,\n",
    "                \"ModelName\": model_name,\n",
    "                \"VariantName\": \"AllTraffic\",\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Create endpoint (provide name and config)\n",
    "    create_endpoint_response = sm_client.create_endpoint(\n",
    "        EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"statusCode\": 200,\n",
    "        \"body\": json.dumps(\"Created Endpoint!\"),\n",
    "        \"other_key\": \"example_value\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates an IAM role using the iam_helper script. Alternatively, you could provide an existing IAM role to be used by the Lambda function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "iam = boto3.client('iam')\n",
    "\n",
    "# Create lambda role with neccessary policies\n",
    "def create_lambda_role(role_name):\n",
    "    try:\n",
    "        response = iam.create_role(\n",
    "            RoleName = role_name,\n",
    "            AssumeRolePolicyDocument = json.dumps({\n",
    "                \"Version\": \"2012-10-17\",\n",
    "                \"Statement\": [\n",
    "                    {\n",
    "                        \"Effect\": \"Allow\",\n",
    "                        \"Principal\": {\n",
    "                            \"Service\": \"lambda.amazonaws.com\"\n",
    "                        },\n",
    "                        \"Action\": \"sts:AssumeRole\"\n",
    "                    }\n",
    "                ]\n",
    "            }),\n",
    "            Description='Role for Lambda to call ECS Fargate task'\n",
    "        )\n",
    "\n",
    "        role_arn = response['Role']['Arn']\n",
    "\n",
    "        response = iam.attach_role_policy(\n",
    "            RoleName = role_name,\n",
    "            PolicyArn = 'arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'\n",
    "        )\n",
    "\n",
    "        response = iam.attach_role_policy(\n",
    "            RoleName = role_name,\n",
    "            PolicyArn = 'arn:aws:iam::aws:policy/AmazonSageMakerFullAccess'\n",
    "        )\n",
    "        \n",
    "        policy_json = {\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [\n",
    "                {\n",
    "                    \"Effect\": \"Allow\",\n",
    "                    \"Action\": [\n",
    "                        \"iam:CreateRole\",\n",
    "                        \"iam:PutRolePolicy\",\n",
    "                        \"iam:AttachRolePolicy\",\n",
    "                        \"iam:DetachRolePolicy\",\n",
    "                        \"iam:GetRole\",\n",
    "                        \"lambda:CreateFunction\",\n",
    "                        \"lambda:InvokeAsync\",\n",
    "                        \"lambda:InvokeFunction\",\n",
    "                        \"iam:PassRole\",\n",
    "                        \"lambda:UpdateAlias\",\n",
    "                        \"lambda:CreateAlias\",\n",
    "                        \"lambda:GetFunctionConfiguration\",\n",
    "                        \"lambda:AddPermission\",\n",
    "                        \"lambda:UpdateFunctionCode\"\n",
    "                    ],\n",
    "                    \"Resource\": [\n",
    "                        \"*\"\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        policy_res = iam.create_policy(\n",
    "        PolicyName='CreateRole',\n",
    "        PolicyDocument=json.dumps(policy_json)\n",
    "        )\n",
    "        policy_arn = policy_res['Policy']['Arn']\n",
    "        \n",
    "        response = iam.attach_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyArn=policy_arn\n",
    "        )\n",
    "\n",
    "        return role_arn\n",
    "\n",
    "    except iam.exceptions.EntityAlreadyExistsException:\n",
    "        print(f'Using ARN from existing role: {role_name}')\n",
    "        response = iam.get_role(RoleName=role_name)\n",
    "        return response['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ARN from existing role: lambda-deployment-role\n"
     ]
    }
   ],
   "source": [
    "lambda_role = create_lambda_role(\"lambda-deployment-role\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the Lambda step. After defining some object names, we use the Lambda helper class to create the actual Lambda function, then pass it to the Lambda step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the current time to define unique names for the resources created\n",
    "current_time = time.strftime(\"%m-%d-%H-%M-%S\", time.localtime())\n",
    "\n",
    "model_name = \"demo-hf-model\" + current_time\n",
    "endpoint_config_name = \"demo-hf-endpoint-config\" + current_time\n",
    "endpoint_name = \"demo-hf-endpoint-\" + current_time\n",
    "function_name = \"sagemaker-demo-hf-lambda-step\" + current_time\n",
    "\n",
    "# Lambda helper class can be used to create the Lambda function\n",
    "func = Lambda(\n",
    "    function_name=function_name, # name of lambda function\n",
    "    execution_role_arn=lambda_role, # IAM lambda role \n",
    "    script=\"lambda_deployer.py\", # Path of Lambda function script\n",
    "    handler=\"lambda_deployer.lambda_handler\", # Lambda handler specified as \"lambda_script.lambda_handler\"\n",
    "    timeout=600, # Maximum time the Lambda function can run before the lambda step fails\n",
    "    memory_size=10240, # Amount of memory in MB a lambda function can use at runtime\n",
    ")\n",
    "\n",
    "# The dictionary returned by the Lambda function is captured by LambdaOutput, \n",
    "# each key in the dictionary corresponds to a LambdaOutput\n",
    "\n",
    "output_param_1 = LambdaOutput(output_name=\"statusCode\", output_type=LambdaOutputTypeEnum.String)\n",
    "output_param_2 = LambdaOutput(output_name=\"body\", output_type=LambdaOutputTypeEnum.String)\n",
    "output_param_3 = LambdaOutput(output_name=\"other_key\", output_type=LambdaOutputTypeEnum.String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The inputs provided to the Lambda function can be retrieved via the `event` object within the `lambda_handler` function\n",
    "# in the Lambda\n",
    "step_deploy_lambda = LambdaStep(\n",
    "    name=\"LambdaStepHuggingFaceDeploy\", # The name of the lambda step\n",
    "    lambda_func=func, # An instance of sagemaker.lambda_helper.Lambda. LambdaStep just invokes the function\n",
    "    inputs={ # Input arguments that will be provided to the lambda function\n",
    "        \"model_name\": model_name,\n",
    "        \"endpoint_config_name\": endpoint_config_name,\n",
    "        \"endpoint_name\": endpoint_name,\n",
    "        \"model_package_arn\": step_register.steps[0].properties.ModelPackageArn,\n",
    "        \"role\": role,\n",
    "    },\n",
    "    outputs=[output_param_1, output_param_2, output_param_3], # List of outputs from the lambda function\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condition for deployment\n",
    "For the condition to be True and the steps after evaluation to run, the evaluated loss of the Hugging Face model must be less than 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The class JsonGet has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "cond_lte = ConditionLessThanOrEqualTo(\n",
    "    left=JsonGet( # Get JSON properties from PropertyFiles\n",
    "        step=step_eval, # The step name from which to get the property file\n",
    "        property_file=evaluation_report, # Either a PropertyFile instance or the name of a property file\n",
    "        json_path=\"eval_loss\", # The JSON path expression to the requested value\n",
    "    ), # The execution variable, parameter, property, or Python primitive value to use in the comparison\n",
    "    right=0.3, # The execution variable, parameter, property, or Python primitive value to compare to\n",
    ")\n",
    "\n",
    "step_cond = ConditionStep(\n",
    "    name=\"CheckHuggingfaceEvalLoss\", # The name of the condition step\n",
    "    conditions=[cond_lte], # A list of sagemaker.workflow.conditions.Condition instances\n",
    "    if_steps=[step_register, step_deploy_lambda], # A list of sagemaker.workflow.steps.Step instances that are marked as ready for execution if the list of conditions evaluates to True\n",
    "    else_steps=[], # nothing to execute, breaks the pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline definition and execution\n",
    "SageMaker Pipelines constructs the pipeline graph from the implicit definition created by the way pipeline steps inputs and outputs are specified. There's no need to specify that a step is a \"parallel\" or \"serial\" step. Steps such as model registration after the condition step are not listed in the pipeline definition because they do not run unless the condition is true. If so, they are run in order based on their specified inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    name=\"HuggingFaceDemoPipeline\", # The name of the pipeline\n",
    "    parameters=[ # The list of the parameters\n",
    "        processing_instance_type,\n",
    "        processing_instance_count,\n",
    "        training_instance_type,\n",
    "        training_instance_count,\n",
    "    ],\n",
    "    steps=[step_process, step_train, step_eval, step_cond], # The list of the non-conditional steps associated with the pipeline\n",
    "    sagemaker_session=sagemaker_session, # Session object that manages interactions with Amazon SageMaker APIs and any other AWS services needed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the pipeline definition in JSON format. You also can inspect the pipeline graph in SageMaker Studio by going to the page for your pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Version': '2020-12-01',\n",
       " 'Metadata': {},\n",
       " 'Parameters': [{'Name': 'ProcessingInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.c5.2xlarge'},\n",
       "  {'Name': 'ProcessingInstanceCount', 'Type': 'Integer', 'DefaultValue': 1},\n",
       "  {'Name': 'TrainingInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.p3.2xlarge'},\n",
       "  {'Name': 'TrainingInstanceCount', 'Type': 'Integer', 'DefaultValue': 1}],\n",
       " 'PipelineExperimentConfig': {'ExperimentName': {'Get': 'Execution.PipelineName'},\n",
       "  'TrialName': {'Get': 'Execution.PipelineExecutionId'}},\n",
       " 'Steps': [{'Name': 'ProcessDataForTraining',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': {'Get': 'Parameters.ProcessingInstanceType'},\n",
       "      'InstanceCount': {'Get': 'Parameters.ProcessingInstanceCount'},\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '492215442770.dkr.ecr.eu-central-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3',\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/preprocessing.py']},\n",
       "    'RoleArn': 'arn:aws:iam::910022457801:role/service-role/AmazonSageMaker-ExecutionRole-20210409T122209',\n",
       "    'ProcessingInputs': [{'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-eu-central-1-910022457801/hugging-face-pipeline-demo/preprocessin-2021-10-29-11-54-55-290/input/code/preprocessing.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'train',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://sagemaker-eu-central-1-910022457801/hugging-face-pipeline-demo/data/train',\n",
       "        'LocalPath': '/opt/ml/processing/train',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'test',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://sagemaker-eu-central-1-910022457801/hugging-face-pipeline-demo/data/test',\n",
       "        'LocalPath': '/opt/ml/processing/test',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'validation',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://sagemaker-eu-central-1-910022457801/hugging-face-pipeline-demo/data/test',\n",
       "        'LocalPath': '/opt/ml/processing/validation',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}},\n",
       "   'CacheConfig': {'Enabled': False, 'ExpireAfter': '30d'}},\n",
       "  {'Name': 'TrainHuggingFaceModel',\n",
       "   'Type': 'Training',\n",
       "   'Arguments': {'AlgorithmSpecification': {'TrainingInputMode': 'File',\n",
       "     'TrainingImage': '763104351884.dkr.ecr.eu-central-1.amazonaws.com/huggingface-pytorch-training:1.7.1-transformers4.6.1-gpu-py36-cu110-ubuntu18.04',\n",
       "     'EnableSageMakerMetricsTimeSeries': True},\n",
       "    'OutputDataConfig': {'S3OutputPath': 's3://sagemaker-eu-central-1-910022457801/'},\n",
       "    'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
       "    'ResourceConfig': {'InstanceCount': {'Get': 'Parameters.TrainingInstanceCount'},\n",
       "     'InstanceType': {'Get': 'Parameters.TrainingInstanceType'},\n",
       "     'VolumeSizeInGB': 30},\n",
       "    'RoleArn': 'arn:aws:iam::910022457801:role/service-role/AmazonSageMaker-ExecutionRole-20210409T122209',\n",
       "    'InputDataConfig': [{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': {'Get': \"Steps.ProcessDataForTraining.ProcessingOutputConfig.Outputs['train'].S3Output.S3Uri\"},\n",
       "        'S3DataDistributionType': 'FullyReplicated'}},\n",
       "      'ChannelName': 'train'},\n",
       "     {'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': {'Get': \"Steps.ProcessDataForTraining.ProcessingOutputConfig.Outputs['test'].S3Output.S3Uri\"},\n",
       "        'S3DataDistributionType': 'FullyReplicated'}},\n",
       "      'ChannelName': 'test'}],\n",
       "    'HyperParameters': {'model_name': '\"distilbert-base-uncased\"',\n",
       "     'train_batch_size': '32',\n",
       "     'epochs': '1',\n",
       "     'sagemaker_submit_directory': '\"s3://sagemaker-eu-central-1-910022457801/hugging-face-pipeline-demo/training-2021-10-29-11-54-55-495/source/sourcedir.tar.gz\"',\n",
       "     'sagemaker_program': '\"train.py\"',\n",
       "     'sagemaker_container_log_level': '20',\n",
       "     'sagemaker_job_name': '\"hugging-face-pipeline-demo/training-2021-10-29-11-54-55-495\"',\n",
       "     'sagemaker_region': '\"eu-central-1\"'},\n",
       "    'DebugHookConfig': {'S3OutputPath': 's3://sagemaker-eu-central-1-910022457801/',\n",
       "     'CollectionConfigurations': []},\n",
       "    'ProfilerRuleConfigurations': [{'RuleConfigurationName': 'ProfilerReport-1635508495',\n",
       "      'RuleEvaluatorImage': '482524230118.dkr.ecr.eu-central-1.amazonaws.com/sagemaker-debugger-rules:latest',\n",
       "      'RuleParameters': {'rule_to_invoke': 'ProfilerReport'}}],\n",
       "    'ProfilerConfig': {'S3OutputPath': 's3://sagemaker-eu-central-1-910022457801/'}},\n",
       "   'CacheConfig': {'Enabled': False, 'ExpireAfter': '30d'}},\n",
       "  {'Name': 'HuggingfaceEvalLoss',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': {'Get': 'Parameters.ProcessingInstanceType'},\n",
       "      'InstanceCount': 1,\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '763104351884.dkr.ecr.eu-central-1.amazonaws.com/huggingface-pytorch-training:1.7.1-transformers4.6.1-gpu-py36-cu110-ubuntu18.04',\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/evaluate.py']},\n",
       "    'RoleArn': 'arn:aws:iam::910022457801:role/service-role/AmazonSageMaker-ExecutionRole-20210409T122209',\n",
       "    'ProcessingInputs': [{'InputName': 'input-1',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': 'Steps.TrainHuggingFaceModel.ModelArtifacts.S3ModelArtifacts'},\n",
       "       'LocalPath': '/opt/ml/processing/model',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-eu-central-1-910022457801/hugging-face-pipeline-demo/evaluation-2021-10-29-11-54-55-593/input/code/evaluate.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'evaluation',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://sagemaker-eu-central-1-910022457801/hugging-face-pipeline-demo/evaluation_report',\n",
       "        'LocalPath': '/opt/ml/processing/evaluation',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}},\n",
       "   'CacheConfig': {'Enabled': False, 'ExpireAfter': '30d'},\n",
       "   'PropertyFiles': [{'PropertyFileName': 'HuggingFaceEvaluationReport',\n",
       "     'OutputName': 'evaluation',\n",
       "     'FilePath': 'evaluation.json'}]},\n",
       "  {'Name': 'CheckHuggingfaceEvalLoss',\n",
       "   'Type': 'Condition',\n",
       "   'Arguments': {'Conditions': [{'Type': 'LessThanOrEqualTo',\n",
       "      'LeftValue': {'Std:JsonGet': {'PropertyFile': {'Get': 'Steps.HuggingfaceEvalLoss.PropertyFiles.HuggingFaceEvaluationReport'},\n",
       "        'Path': 'eval_loss'}},\n",
       "      'RightValue': 0.3}],\n",
       "    'IfSteps': [{'Name': 'HuggingFaceRegisterModel',\n",
       "      'Type': 'RegisterModel',\n",
       "      'Arguments': {'ModelPackageGroupName': 'HuggingFaceModelPackageGroup',\n",
       "       'InferenceSpecification': {'Containers': [{'Image': '763104351884.dkr.ecr.eu-central-1.amazonaws.com/huggingface-pytorch-inference:1.7.1-transformers4.6.1-gpu-py36-cu110-ubuntu18.04',\n",
       "          'Environment': {'SAGEMAKER_PROGRAM': '',\n",
       "           'SAGEMAKER_SUBMIT_DIRECTORY': '',\n",
       "           'SAGEMAKER_CONTAINER_LOG_LEVEL': '20',\n",
       "           'SAGEMAKER_REGION': 'eu-central-1'},\n",
       "          'ModelDataUrl': {'Get': 'Steps.TrainHuggingFaceModel.ModelArtifacts.S3ModelArtifacts'}}],\n",
       "        'SupportedContentTypes': ['application/json'],\n",
       "        'SupportedResponseMIMETypes': ['application/json'],\n",
       "        'SupportedRealtimeInferenceInstanceTypes': ['ml.g4dn.xlarge',\n",
       "         'ml.m5.xlarge'],\n",
       "        'SupportedTransformInstanceTypes': ['ml.g4dn.xlarge', 'ml.m5.xlarge']},\n",
       "       'ModelApprovalStatus': 'Approved'}},\n",
       "     {'Name': 'LambdaStepHuggingFaceDeploy',\n",
       "      'Type': 'Lambda',\n",
       "      'Arguments': {'model_name': 'demo-hf-model10-29-11-54-20',\n",
       "       'endpoint_config_name': 'demo-hf-endpoint-config10-29-11-54-20',\n",
       "       'endpoint_name': 'demo-hf-endpoint-10-29-11-54-20',\n",
       "       'model_package_arn': {'Get': 'Steps.HuggingFaceRegisterModel.ModelPackageArn'},\n",
       "       'role': 'arn:aws:iam::910022457801:role/service-role/AmazonSageMaker-ExecutionRole-20210409T122209',\n",
       "       'bucket': 'sagemaker-eu-central-1-910022457801',\n",
       "       's3_prefix': 'hugging-face-pipeline-demo'},\n",
       "      'FunctionArn': 'arn:aws:lambda:eu-central-1:910022457801:function:sagemaker-demo-hf-lambda-step10-29-11-54-20',\n",
       "      'OutputParameters': [{'OutputName': 'statusCode',\n",
       "        'OutputType': 'String'},\n",
       "       {'OutputName': 'body', 'OutputType': 'String'},\n",
       "       {'OutputName': 'other_key', 'OutputType': 'String'}]}],\n",
       "    'ElseSteps': []}}]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json.loads(pipeline.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:eu-central-1:910022457801:pipeline/huggingfacedemopipeline',\n",
       " 'ResponseMetadata': {'RequestId': '8fcba1df-6ab9-402c-a917-f92d3533202b',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '8fcba1df-6ab9-402c-a917-f92d3533202b',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '94',\n",
       "   'date': 'Fri, 29 Oct 2021 11:54:57 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates a pipeline or updates it, if it already exists\n",
    "pipeline.upsert(role_arn=role) # The role arn that is assumed by workflow to create step artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starts a Pipeline execution in the Workflow service\n",
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waits for the pipeline to complete execution\n",
    "execution.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting predictions from the endpoint\n",
    "After the previous cell completes, you can check whether the endpoint has finished deploying. In SageMaker Studio, click on the tilted triange icon in the left toolbar, then select Endpoints in the drop down menu. In the list of endpoints, click on the one with the name beginning demo-hf-endpoint, then click on AWS settings. When the Status becomes InService, you can run the following code cells.\n",
    "\n",
    "## Setup\n",
    "We can use the endpoint name to set up a Predictor object that will be used to get predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "\n",
    "hf_predictor = sagemaker.predictor.Predictor(endpoint_name, # Name of the Amazon SageMaker endpoint to which requests are sent\n",
    "                                             sagemaker_session=sagemaker_session, # A SageMaker Session object, used for SageMaker interactions\n",
    "                                             serializer=JSONSerializer(), # A serializer object, used to encode data for an inference endpoint\n",
    "                                             deserializer=JSONDeserializer() # A deserializer object, used to decode data from an inference endpoint\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data\n",
    "Here are a couple of sample reviews we would like to classify as positive (LABEL_1) or negative (LABEL_0). Demonstrating the power of advanced Transformer-based models such as this Hugging Face model, the model should do quite well even though the reviews are mixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.8424215912818909}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_input1 = {\"inputs\":\"Although the movie had some plot weaknesses, it was engaging. Special effects were mind boggling.  Can't wait to see what this creative team does next.\"}\n",
    "\n",
    "hf_predictor.predict(sentiment_input1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.9616789817810059}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_input2 = {\"inputs\":\"There was some good acting, but the story was ridiculous. The other sequels in this franchise were better.  It's time to take a break from this IP, but if they switch it up for the next one, I'll check it out.\"}\n",
    "\n",
    "hf_predictor.predict(sentiment_input2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the endpoint\n",
    "Once the endpoint is in service, there are several different ways to examine its current configuration and parameters. One way is to use SageMaker Studio to examine the configuration. In the left toolbar, click the tilted triangle icon, select Endpoints from the drop down menu at the top, then click the endpoint name (beginning demo-hf-endpoint). Click the AWS Settings tab and you can review various endpoint metadata.\n",
    "\n",
    "Another way is to use the boto3 SDK for AWS. You can use this to check certain other parameters, such as the current number of instances behind the endpoint. This is done in the code cell below, and might be useful in circumstances such as when you need to do an ad hoc check on endpoint scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current instance count = 1\n"
     ]
    }
   ],
   "source": [
    "endpoint_desc = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "current_instance_count = endpoint_desc[\"ProductionVariants\"][0][\"CurrentInstanceCount\"]\n",
    "print(f\"Current instance count = {current_instance_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup Resources\n",
    "The following cell will delete the resources created by the Lambda function and the Lambda itself. Deleting other resources such as the S3 bucket and the IAM role for the Lambda function are the responsibility of the notebook user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '45ebf0d0-16d5-42f1-a495-376fcc766abb',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '45ebf0d0-16d5-42f1-a495-376fcc766abb',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Fri, 29 Oct 2021 13:17:08 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete the Lambda function\n",
    "func.delete()\n",
    "\n",
    "# Delete the endpoint\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "\n",
    "# Delete the EndpointConfig\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "\n",
    "# Delete the model\n",
    "sm_client.delete_model(ModelName=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Use a Lambda step for lightweight model deployments to SageMaker hosting services\n",
    " - `Lightweight` means deployments to development or test endpoints, or to internal endpoints that aren’t customer-facing or serving high volumes of traffic\n",
    " - Lambda step enables to add serverless compute operations to pipelines for many different kinds of tasks such as lightweight deployments\n",
    " - Use the Lambda step to run serverless tasks or jobs on Lambda such as splitting datasets or sending custom notifications\n",
    " - The Lambda step can perform tasks such as looking up the latest approved model registered in the SageMaker Model Registry after model building is complete, and then updating an endpoint with that model (or creating a new endpoint if one doesn’t already exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Base Python)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-central-1:936697816551:image/python-3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
